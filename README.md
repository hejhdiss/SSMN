# Sparse-Stream Memory Networks (SSMN)

> Member of [MNNN](https://github.com/hejhdiss/MEMORY-NATIVE-NEURAL_NETWORK) Family 


**Revolutionary neural architectures that replace expensive global attention with "continuous ink" of synaptic weights.**

This repository contains two implementations:
1. **Text-Native SSMN**: Language and Memory are unified - stores geometric relationships between concepts
2. **Standard SSMN**: Sliding window attention + neural synaptic memory

Both architectures achieve **O(nÂ·w) complexity** instead of O(nÂ²) for transformers, with **no global KV cache required**.

---

## ğŸ—ï¸ Architecture Overview

### Core Innovation: The MN (Memory-Native) Layer

Instead of searching through past tokens with attention, information flows out of the sliding window and gets **compressed into synaptic weights** that update during the forward pass:

```
Î”W_f = Î·(h_t âŠ— h_{t-1}) - Î»W_f
```

- **Î· (Plasticity)**: Absorbs current context into weights
- **Î» (Decay)**: Prunes old/irrelevant information, prevents bloat

### Brain-Inspired Design

```
ğŸ“Š Layer Distribution:
â”œâ”€ 80% Static Layers â”€â”€â”€â”€â”€â”€â–º Grammar, basic logic (cortex)
â””â”€ 20% Plastic Layers â”€â”€â”€â”€â”€â–º Memory hubs (hippocampus)
```

---

## ğŸ“¦ What's Included

```
.
â”œâ”€â”€ text_native_ssmn.c      # Text-Native SSMN C library
â”œâ”€â”€ text_native_ssmn.py     # Text-Native SSMN Python wrapper
â”œâ”€â”€ ssmn.c                  # Standard SSMN C library
â”œâ”€â”€ ssmn.py                 # Standard SSMN Python wrapper
â”œâ”€â”€ README.md               # This file
â””â”€â”€ USAGE.md                # Detailed usage examples
```

---

## ğŸš€ Quick Start

### 1. Compile C Libraries

**Linux/Mac:**
```bash
# Text-Native SSMN
gcc -shared -fPIC -o text_native_ssmn.so text_native_ssmn.c -lm -O3

# Standard SSMN
gcc -shared -fPIC -o ssmn.so ssmn.c -lm -O3

# Custom SSMN
gcc -shared -fPIC -o ssmn_custom.so ssmn_custom.c -lm -O3
```

**Windows:**
```bash
# Text-Native SSMN
gcc -shared -o text_native_ssmn.dll text_native_ssmn.c -lm -O3

# Standard SSMN
gcc -shared -o ssmn.dll ssmn.c -lm -O3

# Custom SSMN
gcc -shared -o ssmn_custom.dll ssmn_custom.c -lm -O3
```

**Mac (with Homebrew GCC):**
```bash
# Text-Native SSMN
gcc -shared -fPIC -o text_native_ssmn.dylib text_native_ssmn.c -lm -O3

# Standard SSMN
gcc -shared -fPIC -o ssmn.dylib ssmn.c -lm -O3

# Custom SSMN
gcc -shared -fPIC -o ssmn_custom.so ssmn_custom.c -lm -O3
```

### 2. Run Python Demos

**Text-Native SSMN:**
```bash
python text_native_ssmn.py
```

**Standard SSMN:**
```bash
python ssmn.py
```

---

## ğŸ¯ Text-Native SSMN

### Key Features

âœ¨ **Neural Semantic Encoder**: Converts tokens â†’ "thought embeddings" capturing intent  
ğŸªŸ **Sliding Window Attention**: O(nÂ·w) local context  
ğŸ§  **Semantic Anchors**: Only updates important synaptic connections  
ğŸ’¬ **Internal Recurrent Chat**: Model re-reads its own synaptic state  
ğŸ”— **Unified Memory**: Language IS memory - geometric concept relationships  

### Architecture Flow

```
Token ID
   â†“
[Neural Semantic Encoder] â”€â”€â–º Thought Embedding
   â†“
[Sliding Window Attention] â”€â”€â–º Local Context
   â†“
[Static Layers 80%] â”€â”€â”€â”€â”€â”€â”€â”€â–º Grammar & Logic
   â†“
[Plastic Layers 20%] â”€â”€â”€â”€â”€â”€â”€â–º Memory Hubs
   â”‚
   â”œâ”€â–º Update: Î”W_f = Gate(Importance)Â·[Î·(hâŠ—h_prev) - Î»W_f]
   â”‚
   â””â”€â–º Internal Chat: Re-read synaptic state
   â†“
Output Probabilities
```

### Python Example

```python
from text_native_ssmn import TextNativeSSMN

# Create network
net = TextNativeSSMN(
    vocab_size=10000,
    embed_dim=128,
    hidden_dim=256,
    window_size=512,
    plasticity_eta=0.01,
    decay_lambda=0.001
)

# Process sequence
tokens = [42, 123, 7, 891, 34]
probs = net.generate(tokens)

# Generate text
generated = net.generate_sequence(
    prompt_tokens=[1, 2, 3],
    max_length=100,
    temperature=0.8
)

# Analyze synaptic state
analysis = net.analyze_synaptic_state()
print(f"Synaptic Energy: {analysis['energy']:.6f}")
print(f"Active Modes: {analysis['num_active_modes']}")
```

---

## ğŸ¯ Standard SSMN

### Key Features

ğŸ‘ï¸ **Sliding Window Attention**: O(nÂ·w) local attention (The Eyes)  
ğŸ§  **Neural Synaptic Memory**: Fast-weight matrix W_f (The Brain)  
ğŸ”„ **Decaying Latent Blocks**: Hybrid attention + synaptic cells  
âš¡ **Linear Complexity**: Process infinite sequences  
ğŸšï¸ **Tunable Plasticity**: Adjust Î· and Î» for different memory behaviors  

### Architecture Flow

```
Input Vector
   â†“
[Input Projection]
   â†“
[Sliding Window Attention] â”€â”€â–º O(nÂ·w) local context
   â†“
[Static Layers 80%] â”€â”€â”€â”€â”€â”€â”€â”€â–º Grammar & Logic
   â†“
[Plastic Layers 20%] â”€â”€â”€â”€â”€â”€â”€â–º Memory Hubs
   â”‚
   â”œâ”€â–º Synaptic Update: Î”W_f = Î·(hâŠ—h_prev) - Î»W_f
   â”‚
   â””â”€â–º Apply W_f: output += W_fÂ·h
   â†“
[Output Projection]
   â†“
Output Vector
```

### Python Example

```python
from ssmn import SparseStreamMemoryNetwork
import numpy as np

# Create network
net = SparseStreamMemoryNetwork(
    input_dim=128,
    hidden_dim=256,
    output_dim=64,
    window_size=512,
    plasticity_eta=0.01,
    decay_lambda=0.001
)

# Train on sequential data
X_train = np.random.randn(1000, 128).astype(np.float32)
y_train = np.random.randn(1000, 64).astype(np.float32)

net.fit(X_train, y_train, epochs=20, verbose=1)

# Make predictions
X_test = np.random.randn(100, 128).astype(np.float32)
predictions = net.predict(X_test)

# Analyze memory
analysis = net.analyze_synaptic_state()
print(f"Synaptic Energy: {analysis['energy']:.6f}")
print(f"Spectral Radius: {analysis['spectral_radius']:.4f}")
```

---

## ğŸ”¬ Key Concepts

### 1. Sliding Window Attention

Instead of every token attending to every other token (O(nÂ²)), each token only attends to its **local neighborhood** (O(nÂ·w)):

```python
# Global Attention (Transformer)
complexity = O(nÂ²)  # Quadratic!

# Sliding Window Attention (SSMN)
complexity = O(nÂ·w)  # Linear! (w is constant)
```

### 2. Synaptic Memory Layer

Information that "falls off" the sliding window doesn't disappear - it gets **compressed into synaptic weights**:

```python
# At each step:
Î”W_f = Î·Â·(h_current âŠ— h_previous) - Î»Â·W_f

# Î· controls how fast new info is absorbed
# Î» controls how fast old info decays
```

### 3. The 80/20 Split

Like the brain (cortex vs hippocampus):

- **80% Static Layers**: Handle grammar, basic logic - don't change during inference
- **20% Plastic Layers**: Memory hubs that adapt via synaptic updates

### 4. Text-Native Design (Text-Native SSMN only)

Traditional LLMs: `Token â†’ Embedding â†’ Processing`

Text-Native SSMN: `Token â†’ Thought Embedding (captures intent) â†’ Semantic Processing`

The model doesn't store words - it stores **geometric relationships between concepts**.

---

## ğŸ“Š Performance Characteristics

### Memory Usage

| Architecture | Per-Token Memory | Total Memory |
|--------------|------------------|--------------|
| Transformer  | O(n)             | O(nÂ²)        |
| SSMN         | O(1)             | O(n)         |

### Computational Complexity

| Operation          | Transformer | SSMN    |
|--------------------|-------------|---------|
| Attention          | O(nÂ²)       | O(nÂ·w)  |
| Synaptic Update    | -           | O(dÂ²)   |
| Total per token    | O(nÂ²)       | O(nÂ·w)  |

Where:
- `n` = sequence length
- `w` = window size (constant, e.g., 512)
- `d` = hidden dimension

---

## ğŸ›ï¸ Hyperparameters

### Text-Native SSMN

```python
vocab_size       # Vocabulary size
embed_dim        # Semantic embedding dimension (default: 128)
hidden_dim       # Hidden state dimension (default: 256)
window_size      # Sliding window size (default: 512)
plasticity_eta   # Î· - plasticity rate (default: 0.01)
decay_lambda     # Î» - decay rate (default: 0.001)
```

**Tuning Tips:**
- â†‘ `embed_dim`: Better semantic representation, more memory
- â†‘ `window_size`: More local context, slower
- â†‘ `plasticity_eta`: Faster learning, more volatile memory
- â†‘ `decay_lambda`: Faster forgetting, more stable

### Standard SSMN

```python
input_dim        # Input vector dimension
hidden_dim       # Hidden state dimension (default: 256)
output_dim       # Output vector dimension
window_size      # Sliding window size (default: 512)
plasticity_eta   # Î· - plasticity rate (default: 0.01)
decay_lambda     # Î» - decay rate (default: 0.001)
```

**Tuning Tips:**
- â†‘ `hidden_dim`: More capacity, slower
- â†‘ `window_size`: Better local context, more memory
- Balance `Î·` and `Î»`: High Î· + low Î» = long memory, Low Î· + high Î» = short memory

---

## ğŸ” Monitoring & Debugging

### Key Statistics

Both implementations provide real-time statistics:

```python
# Synaptic memory health
net.synaptic_energy          # How much info is stored
net.synaptic_update_magnitude # How fast memory is changing

# Attention patterns
net.attention_entropy        # Attention distribution spread

# Window state
net.window_fill              # How full is the window

# Full analysis
analysis = net.analyze_synaptic_state()
# Returns: energy, active_modes, spectral_radius, sparsity, etc.
```

### Common Issues

**Problem**: Synaptic energy exploding  
**Solution**: Decrease `plasticity_eta` or increase `decay_lambda`

**Problem**: Network "forgetting" too fast  
**Solution**: Decrease `decay_lambda` or increase `plasticity_eta`

**Problem**: Low attention entropy (peaked distribution)  
**Solution**: Check input normalization, adjust window size

**Problem**: Spectral radius > 1  
**Solution**: Decrease `plasticity_eta` (network may be unstable)

---

## ğŸ†š Comparison: Text-Native vs Standard SSMN

| Feature                    | Text-Native SSMN | Standard SSMN |
|----------------------------|------------------|---------------|
| **Input Type**             | Token IDs        | Continuous vectors |
| **Semantic Encoding**      | âœ… Yes           | âŒ No          |
| **Importance Gating**      | âœ… Yes           | âŒ No          |
| **Internal Chat**          | âœ… Yes           | âŒ No          |
| **Best For**               | NLP, chatbots    | Time series, RL |
| **Memory Selectivity**     | High (gated)     | Medium         |
| **Complexity**             | Higher           | Lower          |

**When to use Text-Native SSMN:**
- Building language models
- Chat/dialogue systems
- Semantic reasoning tasks
- When you need the model to "understand" intent

**When to use Standard SSMN:**
- Time series prediction
- Reinforcement learning
- Control systems
- Continuous data streams

---

## ğŸ“š References

### Theoretical Foundations

1. **Sliding Window Attention**: Reduces complexity from O(nÂ²) to O(nÂ·w)
2. **Fast Weights / Hebbian Learning**: Outer product updates `h_t âŠ— h_{t-1}`
3. **Hippocampus-Cortex Model**: 80/20 static/plastic layer split
4. **Eigenvalue Stability**: Spectral radius monitoring prevents explosion

---

## ğŸ› ï¸ Troubleshooting

### Compilation Issues

**Error**: `library not found`  
**Fix**: Ensure compiled library is in same directory as .py file

**Error**: `undefined symbol`  
**Fix**: Check that `-lm` flag is included (links math library)

### Runtime Issues

**Error**: `ValueError: Expected input_dim=X`  
**Fix**: Check input shape matches network configuration

**Error**: Numerical instability  
**Fix**: Reduce `plasticity_eta` or add gradient clipping

**Warning**: Slow performance  
**Fix**: Compile with `-O3` optimization flag

---

## ğŸ“ License

GPL V3

---

**Built with â¤ï¸ for efficient, brain-inspired AI**
